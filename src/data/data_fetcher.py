import yfinance as yf
import pandas as pd
import wikipedia
import os
import time
from datetime import datetime

def get_sp500_tickers():
    """
    R√©cup√®re la liste officielle S&P 500 via l'API Wikipedia.
    Utilise l'API pour √©viter les erreurs de scraping direct.
    """
    page_title = "List of S&P 500 companies"
    
    try:
        print(f"[{datetime.now()}] üîç Interrogation de l'API Wikipedia pour '{page_title}'...")
        
        # 1. Chargement de la page via l'API (auto_suggest=False √©vite les confusions)
        page = wikipedia.page(page_title, auto_suggest=False)
        
        # 2. R√©cup√©ration du HTML
        html_content = page.html()
        
        # 3. Parsing des tables avec Pandas
        dfs = pd.read_html(html_content)
        
        # La table principale est la premi√®re
        df_sp500 = dfs[0]
        
        # 4. Extraction et Nettoyage
        tickers = df_sp500['Symbol'].tolist()
        
        # Yahoo Finance utilise des tirets (-) au lieu des points (.)
        # Ex: Berkshire Hathaway est BRK.B sur Wiki mais BRK-B sur Yahoo
        tickers = [symbol.replace('.', '-') for symbol in tickers]
        
        print(f"[{datetime.now()}] ‚úÖ {len(tickers)} tickers r√©cup√©r√©s depuis Wikip√©dia.")
        return tickers

    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration Wiki ({e}). Utilisation de la liste de secours.")
        # Fallback si Wikipedia √©choue
        return ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'JPM', 'V', 'PG']

def fetch_data(tickers, period="2y"):
    """
    R√©cup√®re les donn√©es OHLCV de mani√®re s√©quentielle pour √©viter le blocage IP.
    """
    print(f"[{datetime.now()}] üöÄ D√©marrage du t√©l√©chargement pour {len(tickers)} tickers...")
    
    all_data = []
    
    for i, ticker in enumerate(tickers):
        try:
            # Indicateur de progression visuel
            print(f"[{i+1}/{len(tickers)}] Downloading {ticker}...", end=" ")
            
            # Utilisation de Ticker() individuel (plus robuste que .download group√©)
            dat = yf.Ticker(ticker)
            df = dat.history(period=period)
            
            if df.empty:
                print("‚ö†Ô∏è Vide (Ignor√©)")
                continue
            
            # On ne garde que la 'Close'
            df = df[['Close']]
            df.columns = [ticker] # Renomme la colonne
            all_data.append(df)
            print("‚úÖ OK")
            
            # Pause pour respecter le rate limit de Yahoo (Anti-ban)
            time.sleep(0.2) 
            
        except Exception as e:
            print(f"‚ùå Erreur: {e}")

    if not all_data:
        raise ValueError("Aucune donn√©e n'a pu √™tre r√©cup√©r√©e.")

    print(f"[{datetime.now()}] Fusion des donn√©es...")
    combined_df = pd.concat(all_data, axis=1)
    
    # Nettoyage final : Forward Fill puis Backward Fill pour les jours f√©ri√©s/manquants
    if combined_df.isnull().values.any():
        print("Warning: NaNs d√©tect√©s. Application d'un fill.")
        combined_df = combined_df.ffill().bfill()
        
    return combined_df

def save_data(df, path="data/raw/market_data.csv"):
    """Sauvegarde les donn√©es en CSV."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path)
    print(f"[{datetime.now()}] üíæ Donn√©es sauvegard√©es dans {path} | Shape: {df.shape}")

if __name__ == "__main__":
    # 1. R√©cup√©ration de la liste √† jour
    sp500_tickers = get_sp500_tickers()
    
    if sp500_tickers:
        # NOTE : Pour tester rapidement, on prend seulement les 50 premiers tickers.
        # En production, enl√®ve le "[:50]" pour tout t√©l√©charger (√ßa prendra ~3-4 minutes).
        tickers_to_download = sp500_tickers[:500]
        
        # 2. T√©l√©chargement et Sauvegarde
        df = fetch_data(tickers_to_download)
        save_data(df)